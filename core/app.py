import argparse, os
from huggingface_hub import login
from core.embeddings import fetch_embedding, build_vectorstore
from core.model import load_llm
from core.graph import build_graph

def initialize():
    """
    Fetches HuggingFace API key, logs into HuggingFace Hub.

    Raises:
        Exception: Raised when user does not have API Key defined under HF_PWC_CHATBOT_APIKEY.
    """

    print("Initializing...")
    HF_API_KEY = os.environ.get("HF_PWC_CHATBOT_APIKEY")
    if HF_API_KEY:
        login(token=HF_API_KEY)
    else:
        raise Exception("API Key for HuggingFace was not found.")

def rag_chain_executor(rag_chain, question: str):
    """
    Execute the RAG pipeline with a given question.

    Args:
        rag_chain: The compiled LangGraph RAG pipelin
        question (str): The user's input question.

    Returns:
        str: The final answer generated by the pipeline.
    """

    state = {"question": question, "context": [], "answer": ""}
    final_state = rag_chain.invoke(state)
    return final_state["answer"]

def main():
    """
    Entry point for running the chatbot.

    - Initializes embeddings, vector store, retriever, LLM and RAG Graph.
    - Can parse CMD string to single-line query.
    """

    initialize()
    cached_embedding = fetch_embedding()
    db = build_vectorstore(cached_embedding)
    retriever = db.as_retriever()
    llm = load_llm()
    rag_chain = build_graph(retriever, llm)

    parser = argparse.ArgumentParser()
    parser.add_argument("--question", help="Ask a single question")
    args = parser.parse_args()

    if args.question:
        print(rag_chain_executor(rag_chain, args.question))
    else:
        while True:
            q = input("Ask me something (or 'exit'): ")
            if q.lower() == "exit":
                break
            print(rag_chain_executor(rag_chain, q))

if __name__ == "__main__":
    main()
