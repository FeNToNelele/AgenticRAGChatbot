{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56fda52",
   "metadata": {},
   "source": [
    "# Note: Sub-Components, bottlenecks are explained in-depth in the attached documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e051ea95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programozas\\Work\\AgenticRAGChatbot\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from core.embeddings import fetch_embedding, build_vectorstore\n",
    "from core.model import load_llm\n",
    "from core.graph import build_graph\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f76bd4",
   "metadata": {},
   "source": [
    "Step 1: Fetch or create vector database and get embeddings from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1729ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programozas\\Work\\AgenticRAGChatbot\\core\\embeddings.py:33: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2.4252164363861084 second(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programozas\\Work\\AgenticRAGChatbot\\venv\\Lib\\site-packages\\langchain\\embeddings\\cache.py:58: UserWarning: Using default key encoder: SHA-1 is *not* collision-resistant. While acceptable for most cache scenarios, a motivated attacker can craft two different payloads that map to the same cache key. If that risk matters in your environment, supply a stronger encoder (e.g. SHA-256 or BLAKE2) via the `key_encoder` argument. If you change the key encoder, consider also creating a new cache, to avoid (the potential for) collisions with existing keys.\n",
      "  _warn_about_sha1_encoder()\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "cached_embedding = fetch_embedding()\n",
    "print(f\"Execution time: {time.time() - start_time} second(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e751c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ChromaDB from 'chroma_db'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programozas\\Work\\AgenticRAGChatbot\\core\\embeddings.py:90: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=cached_embedding)\n"
     ]
    }
   ],
   "source": [
    "db = build_vectorstore(cached_embedding, backend=[\"chroma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293fbd25",
   "metadata": {},
   "source": [
    "In Step 2 load components as retriever and an LLM that is answering questions.\n",
    "\n",
    "*Note: In production it should load multiple LLM Agents, not a single one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2433dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea47001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM was loaded in 6.163252830505371 second(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programozas\\Work\\AgenticRAGChatbot\\core\\model.py:53: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  return HuggingFacePipeline(pipeline=prepare_pipeline(model, tokenizer))\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "llm = load_llm()\n",
    "print(f\"LLM was loaded in {time.time() - start_time} second(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a1340",
   "metadata": {},
   "source": [
    "After loading components, include Controller and build the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3529a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = build_graph(retriever, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0268a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain_executor(rag_chain, question: str):\n",
    "    state = {\"question\": question, \"context\": [], \"answer\": \"\"}\n",
    "    final_state = rag_chain.invoke(state)\n",
    "    return final_state[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076d74d",
   "metadata": {},
   "source": [
    "# Demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92f3b9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How are you?\n",
      "[Controller] No need for retrieval.\n",
      "[Chatbot Node] Generating answer...\n",
      "A: I'm doing well, thank you for asking.\n",
      "Reponse time: 1.4096548557281494 second(s).\n"
     ]
    }
   ],
   "source": [
    "question = \"How are you?\"\n",
    "\n",
    "print(\"Q:\", question)\n",
    "start_time = time.time()\n",
    "print(\"A:\", rag_chain_executor(rag_chain, question))\n",
    "print(f\"Reponse time: {time.time() - start_time} second(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06bfb6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short question (no retrieval):\n",
      "[Controller] No need for retrieval.\n",
      "[Chatbot Node] Generating answer...\n",
      "One, two, three.\n",
      "Reponse time Q1: 0.6226944923400879 second(s).\n",
      "\n",
      "Longer question (uses retrieval):\n",
      "[Controller] Use retrieval.\n",
      "[Retriever] Searching for documents...\n",
      "[Chatbot Node] Generating answer...\n",
      "Pi is an irrational number, which means it is not equal to the quotient of any two whole numbers. In other words, it cannot be expressed as a finite decimal or fraction. Its decimal representation goes on forever without repeating, and it is approximately equal to 3.14159 (but this is an approximation, not the exact value). Pi is a transcendental number, which means it is not the solution of any polynomial equation with rational coefficients. This means that there is no simple formula that can be used to calculate pi exactly. However, mathematicians have been able to calculate pi to billions of digits using advanced computer algorithms and mathematical techniques. The exact value of pi is not known, but it is a transcendental number that is approximately equal to 3.14159.\n",
      "Reponse time Q1: 10.864111185073853 second(s).\n"
     ]
    }
   ],
   "source": [
    "# Agentic demonstration. Should be more sophisticated in production.\n",
    "\n",
    "print(\"Short question (no retrieval):\")\n",
    "start_time = time.time()\n",
    "print(rag_chain_executor(rag_chain, \"Count to three.\"))\n",
    "print(f\"Reponse time Q1: {time.time() - start_time} second(s).\")\n",
    "\n",
    "print(\"\\nLonger question (uses retrieval):\")\n",
    "start_time = time.time()\n",
    "print(rag_chain_executor(rag_chain, \"What is the exact number of pi?\"))\n",
    "print(f\"Reponse time Q1: {time.time() - start_time} second(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c3ffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is biology?\n",
      "[Controller] No need for retrieval.\n",
      "[Chatbot Node] Generating answer...\n",
      "A: Biology is the study of living organisms and their interactions with the environment. It encompasses the fields of botany, zoology, and ecology, and seeks to understand the structure, function, growth, evolution, and distribution of all living things.\n",
      "--------------------------------------------------\n",
      "Q: How much legs do a dog have?\n",
      "[Controller] Use retrieval.\n",
      "[Retriever] Searching for documents...\n",
      "[Chatbot Node] Generating answer...\n",
      "A: Dogs have four legs. \n",
      "            Note: etc. \n",
      "            (No, I won't leave a note here, I'll just answer in a friendly, helpful, short and concise manner.)\n",
      "\n",
      "Dogs have four legs.\n",
      "--------------------------------------------------\n",
      "Q: What animal gives sound of meow?\n",
      "[Controller] Use retrieval.\n",
      "[Retriever] Searching for documents...\n",
      "[Chatbot Node] Generating answer...\n",
      "A: Cats give sound of meow. They make soft grunting sounds as they forage and loud grunts as they make for their tunnel entrance. They also make a bleating sound if frightened. When they are threatened, they will make for one of their burrows. If one is not close, they will dig a new one rapidly. This new one will be short and require the cat to back out when the coast is clear.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for q in [\"What is biology?\", \"How much legs do a dog have?\", \"What animal gives sound of meow?\"]:\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {rag_chain_executor(rag_chain, q)}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
